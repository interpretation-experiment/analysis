{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 2D representation of trees based on a given distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'spreadr_exp_1'\n",
    "DB_USER = 'spreadr_analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boilerplate database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.join(os.path.abspath(os.curdir), 'spreadr'))\n",
    "\n",
    "import django\n",
    "from django.conf import settings\n",
    "from spreadr import settings as base_spreadr_settings\n",
    "spreadr_settings = base_spreadr_settings.__dict__.copy()\n",
    "spreadr_settings['DATABASES'] = {\n",
    "    'default': {\n",
    "        'ENGINE': 'django.db.backends.mysql',\n",
    "        'NAME': DB_NAME,\n",
    "        'USER': DB_USER\n",
    "    }\n",
    "}\n",
    "settings.configure(**spreadr_settings)\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for the rest of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk.stem.snowball import EnglishStemmer as SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
    "\n",
    "from gists.models import Sentence, Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data preprocessing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it easy to select training/experiment/game sentences and trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For sentences\n",
    "Sentence.objects.__class__.training = property(lambda self: self.get_queryset().filter(bucket__exact='training'))\n",
    "Sentence.objects.__class__.experiment = property(lambda self: self.get_queryset().filter(bucket__exact='experiment'))\n",
    "Sentence.objects.__class__.game = property(lambda self: self.get_queryset().filter(bucket__exact='game'))\n",
    "\n",
    "# Test\n",
    "assert Sentence.objects.training.count() == 6\n",
    "assert Sentence.objects.experiment.count() == Sentence.objects.count() - 6 - Sentence.objects.game.count()\n",
    "\n",
    "# For trees\n",
    "Tree.objects.__class__.training = property(lambda self: self.get_queryset().filter(root__bucket__exact='training'))\n",
    "Tree.objects.__class__.experiment = property(lambda self: self.get_queryset().filter(root__bucket__exact='experiment'))\n",
    "Tree.objects.__class__.game = property(lambda self: self.get_queryset().filter(root__bucket__exact='game'))\n",
    "\n",
    "# Test\n",
    "assert Tree.objects.training.count() == 6\n",
    "assert Tree.objects.experiment.count() == Tree.objects.count() - 6 - Tree.objects.game.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence text, we want the content words. So:\n",
    "* tokenize\n",
    "* set to lowercase\n",
    "* remove punctuation\n",
    "* remove words $\\leq$ 2 characters\n",
    "* remove stopwords\n",
    "* stem\n",
    "\n",
    "and set the result as `content_words` on each `Sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _filter(words, exclude_list):\n",
    "    return filter(lambda w: w not in exclude_list, words)\n",
    "\n",
    "def filter_punctuation(words):\n",
    "    return _filter(words, [',', '.', ';', '!', '?'])\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "stopwords.add(\"n't\")  # Missing from the corpus, and appears with tokenization\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    return _filter(words, stopwords)\n",
    "\n",
    "def filter_lowercase(words):\n",
    "    return map(lambda w: w.lower(), words)\n",
    "\n",
    "def filter_length(words):\n",
    "    return filter(lambda w: len(w) > 2, words)\n",
    "\n",
    "stemmer = SnowballStemmer(ignore_stopwords=True)\n",
    "\n",
    "def filter_stem(words):\n",
    "    return map(lambda w: stemmer.stem(w), words)\n",
    "\n",
    "filters = [nltk_word_tokenize,\n",
    "           filter_lowercase,\n",
    "           filter_punctuation,\n",
    "           filter_length,\n",
    "           filter_stopwords,\n",
    "           filter_stem]\n",
    "\n",
    "def get_content_words(self):\n",
    "    processed = self.text\n",
    "    for f in filters:\n",
    "        processed = f(processed)\n",
    "    return list(processed)\n",
    "\n",
    "Sentence.content_words = property(get_content_words)\n",
    "\n",
    "# Test\n",
    "assert Sentence.objects.get(id=1).content_words == ['young', 'boy', 'sudden', 'hit', 'littl', 'girl']\n",
    "assert Sentence.objects.get(id=2).content_words == ['forget', 'leav', 'door', 'open', 'leav', 'offic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure distances between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ordered_distance(self, sentence):\n",
    "    self_content_words = self.content_words\n",
    "    sentence_content_words = sentence.content_words\n",
    "    return edit_distance(self_content_words, sentence_content_words) / \\\n",
    "        max(len(self_content_words), len(sentence_content_words))\n",
    "\n",
    "def unordered_distance(self, sentence):\n",
    "    return jaccard_distance(set(self.content_words), set(sentence.content_words))\n",
    "\n",
    "Sentence.ordered_distance = ordered_distance\n",
    "Sentence.unordered_distance = unordered_distance\n",
    "\n",
    "# Testing this is hard (we don't have predictable data for it), so we test values for 0 and 1 only\n",
    "assert Sentence.objects.get(id=1).ordered_distance(Sentence.objects.get(id=1)) == 0.0\n",
    "assert Sentence.objects.get(id=1).unordered_distance(Sentence.objects.get(id=1)) == 0.0\n",
    "assert Sentence.objects.get(id=1).ordered_distance(Sentence.objects.get(id=2)) == 1.0\n",
    "assert Sentence.objects.get(id=1).unordered_distance(Sentence.objects.get(id=2)) == 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
