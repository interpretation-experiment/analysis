{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cut-replace rate: how much is a sentence cut vs. replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath('../..'))\n",
    "import analysis\n",
    "\n",
    "DB_NAME = 'spreadr_' + os.path.split(os.path.abspath(os.path.curdir))[1]\n",
    "analysis.setup(DB_NAME)\n",
    "print('Database:', DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from nltk.metrics import edit_distance\n",
    "from frozendict import frozendict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from gists.models import Sentence\n",
    "from analysis import settings, transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 Test optimising for known parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Framework setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kept_sentence_ids = Sentence.objects.kept\\\n",
    "    .filter(parent__isnull=False)\\\n",
    "    .values_list('id', flat=True)\n",
    "kept_sentence_ids = list(kept_sentence_ids)\n",
    "\n",
    "def sample_training_sentences(sample_size):\n",
    "    remaining_shuffled_sentence_ids = random.sample(kept_sentence_ids, len(kept_sentence_ids))\n",
    "    sample_sentences = []\n",
    "    while len(sample_sentences) < sample_size:\n",
    "        candidate = Sentence.objects.get(id=remaining_shuffled_sentence_ids.pop())\n",
    "        if candidate.oc_distance(candidate.parent) > 0:\n",
    "            sample_sentences.append(candidate)\n",
    "    assert len(sample_sentences) == sample_size\n",
    "    return sample_sentences\n",
    "\n",
    "def get_complement_test_sentences(sample_sentences):\n",
    "    test_sentences = []\n",
    "    for sid in set(kept_sentence_ids).difference(\n",
    "            [sentence.id for sentence in sample_sentences]):\n",
    "        candidate = Sentence.objects.get(id=sid)\n",
    "        if candidate.oc_distance(candidate.parent) > 0:\n",
    "            test_sentences.append(candidate)\n",
    "    return test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alignments(sentences, parameters):\n",
    "    frozen_parameters = frozendict(parameters)\n",
    "    return [transformations.align_lemmas(s.parent.tokens, s.tokens,\n",
    "                                         parameters=frozen_parameters)\n",
    "            for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def distance(alignment1, alignment2):\n",
    "    seq1A, seq1B = alignment1[:2]\n",
    "    seq2A, seq2B = alignment2[:2]\n",
    "    seq1A = list(map(id, seq1A))\n",
    "    seq1B = list(map(id, seq1B))\n",
    "    seq2A = list(map(id, seq2A))\n",
    "    seq2B = list(map(id, seq2B))\n",
    "    return (edit_distance(seq1A, seq2A) + edit_distance(seq1B, seq2B)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_COMPARE_FACTOR = 1\n",
    "def x2parameters(x):\n",
    "    return frozendict({\n",
    "        'COMPARE_FACTOR': BASE_COMPARE_FACTOR,\n",
    "        'COMPARE_ORIGIN': x[0] * BASE_COMPARE_FACTOR,\n",
    "        'GAP_OPEN': (x[1] + x[2]) * BASE_COMPARE_FACTOR,\n",
    "        'GAP_EXTEND': x[2] * BASE_COMPARE_FACTOR,\n",
    "        'EXCHANGE': None,\n",
    "    })\n",
    "\n",
    "def parameters2x(parameters):\n",
    "    return (np.array([parameters['COMPARE_ORIGIN'],\n",
    "                      parameters['GAP_OPEN'] - parameters['GAP_EXTEND'],\n",
    "                      parameters['GAP_EXTEND']])\n",
    "            / parameters['COMPARE_FACTOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def objective(x, sentences, ref_alignments):\n",
    "    x_alignments = alignments(sentences, x2parameters(x))\n",
    "    distances = []\n",
    "    for ref_as, x_as in zip(ref_alignments, x_alignments):\n",
    "        if len(x_as) == 0:\n",
    "            # Add an empty alignment if there are none\n",
    "            x_as = [([], [])]\n",
    "        # Or use max+mean\n",
    "        distances.append(np.max([distance(ref_a, x_a) for ref_a, x_a\n",
    "                                 in itertools.zip_longest(ref_as,  x_as,\n",
    "                                                          fillvalue=([], []))]))\n",
    "    return np.sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reference_parameters = {\n",
    "    'COMPARE_FACTOR': BASE_COMPARE_FACTOR,\n",
    "    'COMPARE_ORIGIN': -.5,\n",
    "    'GAP_OPEN': -.5,\n",
    "    'GAP_EXTEND': -.1,\n",
    "    'EXCHANGE': None,\n",
    "}\n",
    "\n",
    "x_bounds = [\n",
    "    (-1, -.01), # COMPARE_ORIGIN / COMPARE_FACTOR\n",
    "    (-1, -.01), # (GAP_OPEN - GAP_EXTEND) / COMPARE_FACTOR\n",
    "    (-1, -.01), # GAP_EXTEND / COMPARE_FACTOR\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Local optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_sentences = sample_training_sentences(200)\n",
    "reference_alignments = alignments(sample_sentences, reference_parameters)\n",
    "for _ in range(1):\n",
    "    x0 = [np.random.uniform(*bounds) for bounds in x_bounds]\n",
    "    result = optimize.minimize(\n",
    "        objective, x0,\n",
    "        #method='SLSQP',\n",
    "        bounds=x_bounds,\n",
    "        args=(sample_sentences, reference_alignments),\n",
    "        options={'disp': True, 'maxiter': 500},\n",
    "        callback=print)\n",
    "    print(result)\n",
    "    print(x2parameters(result.x))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Brute force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting one sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "discretization = 10\n",
    "n_dims = len(x_bounds)\n",
    "xs = [np.linspace(start, stop, discretization) for (start, stop) in x_bounds]\n",
    "grids = np.meshgrid(*xs, indexing='ij')\n",
    "values = np.zeros_like(grids[0])\n",
    "\n",
    "sample_sentences = sample_training_sentences(20)\n",
    "reference_alignments = alignments(sample_sentences, reference_parameters)\n",
    "for i, k in ProgressBar(max_value=len(values.flat))(\n",
    "        enumerate(itertools.product(range(discretization), repeat=n_dims))):\n",
    "    values[k] = objective([grids[j][k] for j in range(n_dims)],\n",
    "                          sample_sentences, reference_alignments)\n",
    "\n",
    "fig, axes = plt.subplots(1, discretization,\n",
    "                         figsize=(70, 5), subplot_kw={'projection': '3d'})\n",
    "for i in range(discretization):\n",
    "    axes[i].plot_surface(grids[1][i, :, :], grids[2][i, :, :], values[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then evaluate several sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sample_size(sample_size, xref=parameters2x(reference_parameters)):\n",
    "    n_runs = 10\n",
    "    print()\n",
    "    print('Sample size {}, doing {} runs'.format(sample_size, n_runs))\n",
    "    \n",
    "    worst_objectives = []\n",
    "    for r in range(n_runs):\n",
    "        print()\n",
    "        print('Run', r)\n",
    "        \n",
    "        # Brute force the parameter fitting\n",
    "        discretization = 10\n",
    "        n_dims = len(x_bounds)\n",
    "        xs = [np.linspace(start, stop, discretization) for (start, stop) in x_bounds]\n",
    "        grids = np.meshgrid(*xs, indexing='ij')\n",
    "        values = np.zeros_like(grids[0])\n",
    "        \n",
    "        sample_sentences = sample_training_sentences(sample_size)\n",
    "        reference_alignments = alignments(sample_sentences, x2parameters(xref))\n",
    "        for i, k in ProgressBar(max_value=len(values.flat))(\n",
    "                enumerate(itertools.product(range(discretization), repeat=n_dims))):\n",
    "            values[k] = objective([grids[j][k] for j in range(n_dims)],\n",
    "                                  sample_sentences, reference_alignments)\n",
    "        \n",
    "        min_value = np.min(values)\n",
    "        min_locations = np.where(values == min_value)\n",
    "        print('Min training objective value {}, found in {} points'\n",
    "              .format(min_value, len(min_locations[0])))\n",
    "        \n",
    "        # Test the best parameters found\n",
    "        test_sentences = get_complement_test_sentences(sample_sentences)\n",
    "        print('Testing on the remaining {} sentences'.format(len(test_sentences)))\n",
    "        test_alignments = alignments(test_sentences, x2parameters(xref))\n",
    "        objective_values = []\n",
    "        for k in zip(*min_locations):\n",
    "            x = [grids[0][k], grids[1][k], grids[2][k]]\n",
    "            objective_values.append(objective(x, test_sentences, test_alignments))\n",
    "        print('Worst objective', np.max(objective_values))\n",
    "        worst_objectives.append(np.max(objective_values))\n",
    "    \n",
    "    print()\n",
    "    print('Overall worst objective for {} runs of sample size {} = {}'\n",
    "          .format(n_runs, sample_size, np.max(worst_objectives)))\n",
    "    return np.max(worst_objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_sample_size(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_sample_size(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_sample_size(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_sample_size(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold100_worst_objectives = []\n",
    "gold100_n_runs = 10\n",
    "for i in range(gold100_n_runs):\n",
    "    xref = [np.random.uniform(*bounds) for bounds in x_bounds]\n",
    "    gold100_worst_objectives.append(evaluate_sample_size(100, xref))\n",
    "print('Worst objective for {} random target parameter '\n",
    "      'sets trained with sample size 100 = {}'.format(np.max(gold100_worst_objectives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells me how many gold alignments I need to be able to recover a working set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Why are the objective values always the same (29, 261, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
