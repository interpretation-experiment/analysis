{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature variation in word replacement chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_FIGURES = False\n",
    "STOPWORDS = 'exclude'  # 'include' or 'exclude'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath('../..'))\n",
    "import analysis\n",
    "\n",
    "FIG = os.path.join(os.path.abspath(os.path.curdir), '{}.png')\n",
    "DB_NAME = 'spreadr_' + os.path.split(os.path.abspath(os.path.curdir))[1]\n",
    "analysis.setup(DB_NAME)\n",
    "print('Database:', DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, spatial\n",
    "import seaborn as sb\n",
    "from statsmodels.stats.proportion import multinomial_proportions_confint\n",
    "from progressbar import ProgressBar\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from gists.models import Sentence, Tree\n",
    "\n",
    "from analysis.utils import quantile_interval, pairwise, get_nlp\n",
    "from analysis.features import PoolError\n",
    "\n",
    "sb.set()\n",
    "nlp = get_nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few helper functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additional_features(parent, child):\n",
    "    return {\n",
    "        'token_length': len(parent.tokens),\n",
    "        'content_length': len(parent.content_tokens),\n",
    "    }\n",
    "\n",
    "def base(parent, child):\n",
    "    return {\n",
    "        # indices\n",
    "        'tree_id': parent.tree.id,\n",
    "        'parent_id': parent.id,\n",
    "        'branch_id': parent.head.id if parent.parent is not None else child.head.id,\n",
    "        'child_id': child.id if child is not None else np.nan,\n",
    "        # independent variables\n",
    "        'depth': parent.depth,\n",
    "        'is_root': parent.parent is None,\n",
    "    }\n",
    "\n",
    "def final_feature_name(feature, rel):\n",
    "    return feature + (('-rel' + rel) if rel is not None else '')\n",
    "\n",
    "def hypernym_rate(parent_lemma, child_lemma):\n",
    "    parent_synsets = wordnet.synsets(parent_lemma)\n",
    "    child_synsets = wordnet.synsets(child_lemma)\n",
    "    hypernym_count = 0\n",
    "    hyponym_count = 0\n",
    "    for parent_synset in parent_synsets:\n",
    "        for child_synset in child_synsets:\n",
    "            parent_paths = parent_synset.hypernym_paths()\n",
    "            child_paths = child_synset.hypernym_paths()\n",
    "            hypernym_count += np.mean([child_synset in p for p in parent_paths])\n",
    "            hyponym_count += np.mean([parent_synset in p for p in child_paths])\n",
    "    \n",
    "    if hypernym_count == 0 and hyponym_count == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return hypernym_count / (hypernym_count + hyponym_count)\n",
    "                \n",
    "def rows(parent, child, feature, rel=None):\n",
    "    STOPWORDS_nan = Sentence._SW_NAN if STOPWORDS == Sentence._SW_EXCLUDE else STOPWORDS\n",
    "    parent_values = parent.features(feature, rel=rel, stopwords=STOPWORDS_nan)\n",
    "    child_values = child.features(feature, rel=rel, stopwords=STOPWORDS_nan)\n",
    "    _, _, rpl_pairs, _ = parent.consensus_relationships(child)\n",
    "\n",
    "    for (parent_id, child_id) in rpl_pairs:\n",
    "        # Get the values' quantile position.\n",
    "        parent_start_quantile, parent_stop_quantile = quantile_interval(parent_values, parent_values[parent_id])\n",
    "        child_start_quantile, child_stop_quantile = quantile_interval(child_values, child_values[child_id])\n",
    "        \n",
    "        # Get h0 if possible (h0n will never fail)\n",
    "        try:\n",
    "            h0 = parent.token_average(parent_id, feature, rel=rel, stopwords=STOPWORDS_nan)\n",
    "        except PoolError:\n",
    "            h0 = np.nan\n",
    "\n",
    "        parent_token = parent.tokens[parent_id]\n",
    "        child_token = child.tokens[child_id]\n",
    "        # Compute feature-independent things only once\n",
    "        if feature == 'aoa':\n",
    "            # Get the distance between parent and child tokens\n",
    "            if (parent_token.has_vector and child_token.has_vector and\n",
    "                    (STOPWORDS == Sentence._SW_INCLUDE or (parent_id in parent.content_ids\n",
    "                                                           and child_id in child.content_ids))):\n",
    "                # Both words are stopword-accepted, and have vectors\n",
    "                distance = spatial.distance.cosine(parent_token.vector, child_token.vector)\n",
    "                distances_h0n = []\n",
    "                for syn in Sentence._strict_synonyms(parent_token.lemma_):\n",
    "                    nlp_syn = nlp(syn)[0]\n",
    "                    if nlp_syn.has_vector:\n",
    "                        distances_h0n.append(spatial.distance.cosine(parent_token.vector, nlp_syn.vector))\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "                    distance_h0n = np.mean(distances_h0n)\n",
    "            else:\n",
    "                distance = np.nan\n",
    "                distance_h0n = np.nan\n",
    "            \n",
    "            # Get hypernym relationship and rate\n",
    "            h_rate = hypernym_rate(parent_token.lemma_, child_token.lemma_)\n",
    "            h_related = not np.isnan(h_rate)\n",
    "        else:\n",
    "            distance = np.nan\n",
    "            distance_h0n = np.nan\n",
    "            h_rate = np.nan\n",
    "            h_related = np.nan\n",
    "            h_rate_h0n = np.nan\n",
    "            h_related_h0n = np.nan\n",
    "        \n",
    "        row = base(parent, child)\n",
    "        row.update(additional_features(parent, child))\n",
    "        row.update({\n",
    "            'feature': final_feature_name(feature, rel),\n",
    "            'parent_value': parent_values[parent_id],\n",
    "            'child_value': child_values[child_id],\n",
    "            'parent_pos': parent_token.pos,\n",
    "            'child_pos': child_token.pos,\n",
    "            'distance': distance,\n",
    "            'distance_h0n': distance_h0n,\n",
    "            'hypernym_rate': h_rate,\n",
    "            'hypernym_related': h_related,\n",
    "            'parent_start_quantile': parent_start_quantile,\n",
    "            'parent_stop_quantile': parent_stop_quantile,\n",
    "            'child_start_quantile': child_start_quantile,\n",
    "            'child_stop_quantile': child_stop_quantile,\n",
    "            'h0': h0,\n",
    "            'h0n': parent.token_average(parent_id, feature, rel=rel, stopwords=STOPWORDS_nan,\n",
    "                                        restrict_synonyms=True),\n",
    "        })\n",
    "        # TODO for verbs/nouns/adj-verb: add POS, parent-child distance, h0n distance (avg of synonyms)\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the master DataFrame that we plot below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = sorted(set(Sentence.WORD_FEATURES.keys())\n",
    "                  .difference(['depth_under', 'depth_above',\n",
    "                               'depth_prop', 'depth_subtree_prop',\n",
    "                               'sentence_prop']))\n",
    "features_rel = list(map(lambda f: final_feature_name(f, 'mean'), features))\n",
    "\n",
    "data = []\n",
    "for tree in ProgressBar(max_value=Tree.objects.experiment.count())(Tree.objects.experiment):\n",
    "    # Clean up before each tree, or else memory usage explodes\n",
    "    analysis.utils.Memoize.drop_caches()\n",
    "    for head in tree.root.children.kept:\n",
    "        \n",
    "        for parent, child in pairwise(head.branch_sentences(with_root=True, with_leaf=True)):\n",
    "        \n",
    "            for feature in features:\n",
    "                for row in rows(parent, child, feature, rel=None):\n",
    "                    data.append(row)\n",
    "                for row in rows(parent, child, feature, rel='mean'):\n",
    "                    data.append(row)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "index = sorted(set(data.columns).difference(['value', 'feature']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variations = data.groupby(['child_id', 'feature'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_variation(**kwargs):\n",
    "    data = kwargs.pop('data')\n",
    "    color = kwargs.get('color', 'blue')\n",
    "    quantiles = kwargs.get('quantiles', False)\n",
    "    binning = kwargs.get('binning', 5)\n",
    "    x = data['parent_value']\n",
    "    y = data['child_value']\n",
    "    h0 = data['h0']\n",
    "    h0n = data['h0n']\n",
    "    \n",
    "    # Compute binning.\n",
    "    cut, cut_kws = ((pd.qcut, {}) if quantiles\n",
    "                    else (pd.cut, {'right': False}))\n",
    "    for bin_count in range(binning, 0, -1):\n",
    "        try:\n",
    "            x_bins, bins = cut(x, bin_count, labels=False,\n",
    "                               retbins=True, **cut_kws)\n",
    "            break\n",
    "        except ValueError:\n",
    "            pass\n",
    "    middles = (bins[:-1] + bins[1:]) / 2\n",
    "    \n",
    "    # Compute bin values.\n",
    "    h0s = np.zeros(bin_count)\n",
    "    h0ns = np.zeros(bin_count)\n",
    "    values = np.zeros(bin_count)\n",
    "    cis = np.zeros(bin_count)\n",
    "    for i in range(bin_count):\n",
    "        indices = x_bins == i\n",
    "        n = indices.sum()\n",
    "        h0s[i] = h0[indices].mean()\n",
    "        h0ns[i] = h0n[indices].mean()\n",
    "        values[i] = y[indices].mean()\n",
    "        cis[i] = (stats.t.ppf(.975, n - 1) * y[indices].std(ddof=1)\n",
    "                  / np.sqrt(n))\n",
    "    \n",
    "    # Plot.\n",
    "    nuphi = r'\\nu_{\\phi}'\n",
    "    plt.plot(middles, values, '-', lw=2, color=color,\n",
    "             label='${}$'.format(nuphi))\n",
    "    plt.fill_between(middles, values - cis, values + cis,\n",
    "                     color=sb.desaturate(color, 0.2), alpha=0.2)\n",
    "    plt.plot(middles, h0s, '--', color=sb.desaturate(color, 0.2),\n",
    "             label='${}^0$'.format(nuphi))\n",
    "    plt.plot(middles, h0ns, linestyle='-.',\n",
    "             color=sb.desaturate(color, 0.2),\n",
    "             label='${}^{{00}}$'.format(nuphi))\n",
    "    plt.plot(middles, middles, linestyle='dotted',\n",
    "             color=sb.desaturate(color, 0.2),\n",
    "             label='$y = x$')\n",
    "    lmin, lmax = middles[0], middles[-1]\n",
    "    h0min, h0max = min(h0s.min(), h0ns.min()), max(h0s.max(), h0ns.max())\n",
    "    # Rescale limits if we're touching H0 or H00.\n",
    "    if h0min < lmin:\n",
    "        lmin = h0min - (lmax - h0min) / 10\n",
    "    elif h0max > lmax:\n",
    "        lmax = h0max + (h0max - lmin) / 10\n",
    "    plt.xlim(lmin, lmax)\n",
    "    plt.ylim(lmin, lmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_grid(data, features, plot_function, xlabel, ylabel, plot_kws={}):\n",
    "    g = sb.FacetGrid(data=data, dropna=False,\n",
    "                     sharex=False, sharey=False,\n",
    "                     col='feature', hue='feature',\n",
    "                     col_order=features, hue_order=features,\n",
    "                     col_wrap=5, aspect=1.5, size=3)\n",
    "    g.map_dataframe(plot_function, **plot_kws)\n",
    "    g.set_titles('{col_name}')\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "    for ax in g.axes.ravel():\n",
    "        legend = ax.legend(frameon=True, loc='best')\n",
    "        if not legend:\n",
    "            # Skip if nothing was plotted on these axes.\n",
    "            continue\n",
    "        frame = legend.get_frame()\n",
    "        frame.set_facecolor('#f2f2f2')\n",
    "        frame.set_edgecolor('#000000')\n",
    "        #ax.set_title(Sentence._transformed_word_feature(ax.get_title())\n",
    "        #             .__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature variation on replacement, fixed bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(variations, features, plot_variation, 'parent', 'child',\n",
    "          plot_kws={'binning': 6, 'quantiles': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Sentence-relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(variations, features_rel, plot_variation, 'parent', 'child',\n",
    "          plot_kws={'binning': 6, 'quantiles': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature variation on replacement, quantile bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(variations, features, plot_variation, 'parent', 'child',\n",
    "          plot_kws={'binning': 6, 'quantiles': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Sentence-relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(variations, features_rel, plot_variation, 'parent', 'child',\n",
    "          plot_kws={'binning': 6, 'quantiles': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cumulative transformation distance ~ POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = data[data.feature == 'aoa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.1f}% of all replacements maintain the exact same POS\"\n",
    "      .format(100 * (data1.parent_pos == data1.child_pos).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 For all replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 99, the unknown POS\n",
    "td_pos = data1[data1.parent_pos != 99]\\\n",
    "    .groupby('parent_pos')['distance', 'distance_h0n']\\\n",
    "    .aggregate({'mean': 'mean',\n",
    "                'ci': lambda x: (stats.t.ppf(.975, len(x) - 1) * x.std(ddof=1)\n",
    "                                 / np.sqrt(len(x)))})\\\n",
    "    .rename(index=lambda n: nlp.vocab.strings[int(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = td_pos.loc[:, 'mean'].plot(kind='bar')\n",
    "for i, (_, distance, ci) in enumerate(td_pos[[('mean', 'distance'), ('ci', 'distance')]].itertuples()):\n",
    "    ax.plot([i - .125, i - .125], [max(0, distance - ci), min(1, distance + ci)],\n",
    "            lw=4, color='grey', label='95% CI' if i == 0 else None)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 For replacements that maintain the same POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 99, the unknown POS\n",
    "td_stablepos = data1[(data1.parent_pos == data1.child_pos) & (data1.parent_pos != 99)]\\\n",
    "    .groupby('parent_pos')['distance', 'distance_h0n']\\\n",
    "    .aggregate({'mean': 'mean',\n",
    "                'ci': lambda x: (stats.t.ppf(.975, len(x) - 1) * x.std(ddof=1)\n",
    "                                 / np.sqrt(len(x)))})\\\n",
    "    .rename(index=lambda n: nlp.vocab.strings[int(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = td_stablepos.loc[:, 'mean'].plot(kind='bar')\n",
    "for i, (_, distance, ci) in enumerate(td_stablepos[[('mean', 'distance'), ('ci', 'distance')]].itertuples()):\n",
    "    ax.plot([i - .125, i - .125], [max(0, distance - ci), min(1, distance + ci)],\n",
    "            lw=4, color='grey', label='95% CI' if i == 0 else None)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Hypernym-hyponym rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.1f}% of all replacements feature some sort of hypernym-hyponym relationship\"\n",
    "      .format(100 * data1.hypernym_related.mean()))\n",
    "print(\"When restricting to same-POS replacements, that number is {:.1f}%\"\n",
    "      .format(100 * data1[data1.parent_pos == data1.child_pos].hypernym_related.mean()))\n",
    "print(\"Whereas for different-POS replacements, it is {:.1f}%\"\n",
    "      .format(100 * data1[data1.parent_pos != data1.child_pos].hypernym_related.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 99, the unknown POS\n",
    "hh_pos = data1[data1.parent_pos != 99]\\\n",
    "    .groupby('parent_pos')['hypernym_rate']\\\n",
    "    .aggregate({'hypernym': 'sum',\n",
    "                'related': lambda x: (data1[data1.parent_pos != 99].loc[x.index].hypernym_related).sum(),\n",
    "                'total': 'size'})\\\n",
    "    .rename(index=lambda n: nlp.vocab.strings[int(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hh_pos['unrelated'] = hh_pos.total - hh_pos.related\n",
    "hh_pos['hyponym'] = hh_pos.related - hh_pos.hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hh_pos[['unrelated', 'hypernym', 'hyponym']].plot(kind='bar', width=.75)\n",
    "for i, (_, unrelated, hypernym, hyponym) in enumerate(hh_pos[['unrelated', 'hypernym', 'hyponym']].itertuples()):\n",
    "    total = unrelated + hypernym + hyponym\n",
    "    cis = multinomial_proportions_confint(np.array([unrelated, hypernym, hyponym]).round(),\n",
    "                                          method='goodman')\n",
    "    ax.plot([i-.25, i-.25], cis[0] * total,\n",
    "            lw=4, color='grey', label='95% CI' if i == 0 else None)\n",
    "    ax.plot([i, i], cis[1] * total,\n",
    "            lw=4, color='grey')\n",
    "    ax.plot([i+.25, i+.25], cis[2] * total,\n",
    "            lw=4, color='grey')\n",
    "ax.set_title('Parent to child relationship')\n",
    "ax.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
