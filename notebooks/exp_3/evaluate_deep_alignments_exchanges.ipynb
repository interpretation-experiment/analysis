{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluate 100 alignments with exchanges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_TEST = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: spreadr_exp_3\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath('../..'))\n",
    "import analysis\n",
    "\n",
    "FIG = os.path.join(os.path.abspath(os.path.curdir), '{}.png')\n",
    "DB_NAME = 'spreadr_' + os.path.split(os.path.abspath(os.path.curdir))[1]\n",
    "analysis.setup(DB_NAME)\n",
    "print('Database:', DB_NAME)\n",
    "\n",
    "HAND_ALIGNMENTS_FILE = '../../data/alignments/' + DB_NAME + '/sebastien-lerique.csv'\n",
    "CODINGS_ALIGNMENTS_FILE = ('../../data/codings/'\n",
    "                           + DB_NAME\n",
    "                           + '/alignments'\n",
    "                           + '/sebastien-lerique_on_sebastien-lerique-deep-alignments-exchanges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Imports for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from gists.models import Sentence\n",
    "\n",
    "from analysis.transformations import format_deep_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 Look at 100 non-trivial deep alignments, with exchanges, to code their quality\n",
    "\n",
    "Only look at sentences for which the alignment algorithm has explored subalignments. It may be that exchanging parts of the sentence was less optimal than keeping the original shallow alignments, so not all these sentences have exchanges, but they always have been compared to alignments with exchanges.\n",
    "\n",
    "So coding these alignments tells us 1) if there are any missing exchanges, 2) if the exchanges found are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_sentences(sentence_ids, n):\n",
    "    found = []\n",
    "    while len(found) < n:\n",
    "        sentence_id = sentence_ids.pop()\n",
    "        sentence = Sentence.objects.get(id=sentence_id)\n",
    "        if sentence.parent.ow_distance(sentence) == 0:\n",
    "            # Skip this sentence, it has no interesting transformations\n",
    "            continue\n",
    "        alignments = sentence.parent.align_deep_lemmas(sentence)\n",
    "        #if sum([len(subalignment)\n",
    "        #        for alignment in alignments\n",
    "        #        for subalignment in alignment['subalignments']]) == 0:\n",
    "        #    # Skip this sentence, it has no non-trivial subalignments (i.e. no exchanges)\n",
    "        if sum([len(alignment['subalignments'])\n",
    "                for alignment in alignments]) == 0:\n",
    "            # Skip this sentence, it has no subalignments\n",
    "            continue\n",
    "        found.append(sentence_id)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hand_aligned_ids = []\n",
    "with open(HAND_ALIGNMENTS_FILE, 'r') as hand_alignments_file:\n",
    "    reader = csv.DictReader(hand_alignments_file)\n",
    "    for row in reader:\n",
    "        hand_aligned_ids.append(row['sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3796,\n",
       " 600,\n",
       " 2147,\n",
       " 1675,\n",
       " 772,\n",
       " 900,\n",
       " 1451,\n",
       " 3206,\n",
       " 3174,\n",
       " 2256,\n",
       " 882,\n",
       " 2680,\n",
       " 3440,\n",
       " 3296,\n",
       " 3158,\n",
       " 3212,\n",
       " 149,\n",
       " 3104,\n",
       " 1568,\n",
       " 1403,\n",
       " 940,\n",
       " 3186,\n",
       " 2916,\n",
       " 3156,\n",
       " 185,\n",
       " 3175,\n",
       " 3667,\n",
       " 2843,\n",
       " 1519,\n",
       " 1529,\n",
       " 2907,\n",
       " 3161,\n",
       " 2944,\n",
       " 2464,\n",
       " 578,\n",
       " 1301,\n",
       " 2322,\n",
       " 1093,\n",
       " 254,\n",
       " 2195,\n",
       " 2288,\n",
       " 2118,\n",
       " 2061,\n",
       " 425,\n",
       " 757,\n",
       " 80,\n",
       " 747,\n",
       " 1938,\n",
       " 2838,\n",
       " 1352,\n",
       " 3060,\n",
       " 3070,\n",
       " 2665,\n",
       " 3911,\n",
       " 2351,\n",
       " 2122,\n",
       " 800,\n",
       " 2425,\n",
       " 1503,\n",
       " 36,\n",
       " 3164,\n",
       " 932,\n",
       " 2499,\n",
       " 4029,\n",
       " 2935,\n",
       " 2149,\n",
       " 1216,\n",
       " 377,\n",
       " 2718,\n",
       " 1172,\n",
       " 81,\n",
       " 2253,\n",
       " 314,\n",
       " 667,\n",
       " 2733,\n",
       " 866,\n",
       " 793,\n",
       " 34,\n",
       " 2356,\n",
       " 947,\n",
       " 227,\n",
       " 2123,\n",
       " 1470,\n",
       " 1161,\n",
       " 1278,\n",
       " 2154,\n",
       " 2178,\n",
       " 2231,\n",
       " 3160,\n",
       " 988,\n",
       " 147,\n",
       " 3141,\n",
       " 1157,\n",
       " 1313,\n",
       " 2064,\n",
       " 1581,\n",
       " 3090,\n",
       " 665,\n",
       " 2206,\n",
       " 3032]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_sentence_ids = list(Sentence.objects.kept\n",
    "                         .filter(parent__isnull=False)\n",
    "                         .values_list('id', flat=True))\n",
    "testable_sentence_ids = list(set(kept_sentence_ids).difference(hand_aligned_ids))\n",
    "random.shuffle(testable_sentence_ids)\n",
    "\n",
    "found_sentences = find_sentences(testable_sentence_ids, N_TEST)\n",
    "found_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 Hand-code the alignments for number of errors\n",
    "\n",
    "This is done by hand, and the result is in `CODINGS_ALIGNMENTS_FILE`.\n",
    "\n",
    "I coded alignments for the number of problems for exchanges: +1 for any exchange that was missing, mistaken, or too many.\n",
    "\n",
    "In the vast majority of cases, errors are missing exchanges, either because the score didn't reach high enough (the chunks to match had too many differences, e.g. block reformulations), or because one chunk was counted as exchanged and its own residue should have been exchanged with a higher level chunk (i.e. it would have worked if the chunk was split in two)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4 Counting the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "exchange_errors = []\n",
    "with open(CODINGS_ALIGNMENTS_FILE, 'r') as codings_file:\n",
    "    reader = csv.DictReader(codings_file)\n",
    "    for row in reader:\n",
    "        exchange_errors.append(row['exchange_errors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 sentences have 0 error(s)\n",
      "17 sentences have 1 error(s)\n",
      "2 sentences have 2 error(s)\n"
     ]
    }
   ],
   "source": [
    "for (error_count, count) in Counter(exchange_errors).items():\\\n",
    "    print('{} sentences have {} error(s)'.format(count, error_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
